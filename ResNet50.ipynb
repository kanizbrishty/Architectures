{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xETSS1YXaK2m",
        "outputId": "32cd9b1a-67b8-4a2d-d495-836858fa4647"
      },
      "outputs": [],
      "source": [
        "!nvidia-smi"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q3c7-hU0aOLH",
        "outputId": "66370a95-64ec-41db-ea47-aec917bed6de"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IkCQoVEh6Xsk",
        "outputId": "16891236-ba87-4f27-b5c7-dd37880d177e"
      },
      "outputs": [],
      "source": [
        "pip install keras-resnet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lJ0o_OfcaZ6V"
      },
      "outputs": [],
      "source": [
        "from keras_resnet.models import ResNet50\n",
        "from keras.models import Model\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rHWrSJPHaaCO",
        "outputId": "7ad5aa7e-c1d7-4c37-a7e9-dd50c5c5ec33"
      },
      "outputs": [],
      "source": [
        "#Loading deep learning algorithm\n",
        "from tensorflow import keras\n",
        "import tensorflow\n",
        "from tensorflow.keras.layers import Input, Lambda, Dense, Flatten\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator,load_img\n",
        "from tensorflow.keras.models import Sequential\n",
        "import numpy as np\n",
        "from glob import glob\n",
        "#import keras\n",
        "#import keras.backend as K\n",
        "#from keras.models import Sequential\n",
        "#from keras.layers import Dense\n",
        "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
        "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau\n",
        "\n",
        "IMAGE_SIZE = [224,224]\n",
        "CLASS=4\n",
        "inception = tensorflow.keras.applications.ResNet50(input_shape=IMAGE_SIZE + [3], weights='imagenet', include_top=False)\n",
        "#model = ResNet50(weights='imagenet', include_top=False)\n",
        "for layer in inception.layers:\n",
        "    layer.trainable = False\n",
        "\n",
        "x = Flatten()(inception.output)\n",
        "prediction = Dense(CLASS, activation='softmax')(x)\n",
        "model = Model(inputs=inception.input, outputs=prediction)\n",
        "adam = keras.optimizers.Adam(lr = 0.001)\n",
        "model.compile(\n",
        "  loss='categorical_crossentropy',\n",
        "  optimizer = adam,\n",
        "  metrics=['accuracy']\n",
        ")\n",
        "print(\"\\n\\n\")\n",
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MQw6eyxHaaFy",
        "outputId": "caa14267-dc31-45b1-c7a8-3feb731a8451"
      },
      "outputs": [],
      "source": [
        "train_datagen = ImageDataGenerator(rescale = 1./255,\n",
        "                                   shear_range = 0.2,\n",
        "                                   zoom_range = 0.2,\n",
        "                                   horizontal_flip = True)\n",
        "val_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "training_set = train_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/train',\n",
        "                                                 target_size = (224,224),\n",
        "                                                 batch_size = 32,\n",
        "                                                 class_mode = 'categorical')\n",
        "val_set = val_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/val',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 32,\n",
        "                                            class_mode = 'categorical')\n",
        "test_datagen = ImageDataGenerator(rescale = 1./255)\n",
        "test_set = test_datagen.flow_from_directory('/content/drive/MyDrive/defense/Split-dataset/test',\n",
        "                                            target_size = (224,224),\n",
        "                                            batch_size = 1,\n",
        "                                            class_mode = 'categorical')\n",
        "print(\"\\n\\n\")\n",
        "model.optimizer.get_config()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "3bigtWDUaaIe",
        "outputId": "f14f61c2-e15e-4a83-f53c-ff3fae7a6a2c"
      },
      "outputs": [],
      "source": [
        "filepath = \"/content/drive/MyDrive/defense/ResNet50/highest_val2.h5\"\n",
        "filepath2 = \"/content/drive/MyDrive/defense/ResNet50/highest_train2.h5\"\n",
        "checkpoint1 = ModelCheckpoint(filepath, monitor='val_accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "checkpoint2 = ModelCheckpoint(filepath2, monitor='accuracy', verbose=1,\n",
        "                             save_best_only=True, mode='max')\n",
        "callbacks_list = [checkpoint1,checkpoint2]\n",
        "r = model.fit_generator(\n",
        "    training_set,\n",
        "    epochs=160,\n",
        "    validation_data=val_set,\n",
        "    steps_per_epoch=len(training_set),\n",
        "    validation_steps=len(val_set),\n",
        "    callbacks=callbacks_list\n",
        ")\n",
        "model.save_weights(\"/content/drive/MyDrive/defense/ResNet50/end2.h5\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 562
        },
        "id": "rt7PKjgCaaLP",
        "outputId": "7d16c039-0336-41b5-ca6a-2fd25590e79d"
      },
      "outputs": [],
      "source": [
        "\n",
        "#plot of accuracy and loss\n",
        "\n",
        "import matplotlib.image as mpimg\n",
        "import matplotlib.pyplot as plt\n",
        "# plot the loss\n",
        "plt.plot(r.history['loss'], label='train loss')\n",
        "plt.plot(r.history['val_loss'], label='val loss')\n",
        "plt.title('Training and validation Loss')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('LossVal_loss')\n",
        "# plot the accuracy\n",
        "plt.plot(r.history['accuracy'], label='train acc')\n",
        "plt.plot(r.history['val_accuracy'], label='val acc')\n",
        "plt.title('Training and validation accuracy')\n",
        "plt.legend()\n",
        "plt.show()\n",
        "plt.savefig('AccVal_acc')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2u6uTAxTaaOa",
        "outputId": "011192eb-8e47-44ec-bf2e-9cbec9800aeb"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/keras/engine/training.py:2006: UserWarning: `Model.evaluate_generator` is deprecated and will be removed in a future version. Please use `Model.evaluate`, which supports generators.\n",
            "  warnings.warn('`Model.evaluate_generator` is deprecated and '\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loss = 0.821776807308197\n",
            "Test Accuracy = 0.773809552192688\n"
          ]
        }
      ],
      "source": [
        "#evaluating the model (test acc)\n",
        "#batch size = 32\n",
        "model.load_weights('/content/drive/MyDrive/Onion Diseases/Accuracy /ResNet50/highest_val2.h5')\n",
        "preds = model.evaluate_generator(test_set)\n",
        "print (\"Loss = \" + str(preds[0]))\n",
        "print (\"Test Accuracy = \" + str(preds[1]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 389
        },
        "id": "1HaCDpKca3Yd",
        "outputId": "ffa4b98e-dbbf-4983-e0fc-816affb2a94e"
      },
      "outputs": [],
      "source": [
        "#confusion matrix\n",
        "\n",
        "#you have to set test bath size=1 before running the cell\n",
        "import pandas as pd\n",
        "import seaborn as sn\n",
        "import tensorflow as tf\n",
        "model.load_weights('/content/drive/MyDrive/Onion Diseases/Accuracy /ResNet50/highest_val2.h5')\n",
        "filenames=test_set.filenames\n",
        "nb_samples=len(test_set)\n",
        "y_prob=[]\n",
        "y_act=[]\n",
        "test_set.reset()\n",
        "for _ in range (nb_samples):\n",
        "    X_test,Y_test = test_set.next()\n",
        "    y_prob.append(model.predict(X_test))\n",
        "    y_act.append(Y_test)\n",
        "predicted_class=[list(training_set.class_indices.keys())[i.argmax()] for i in y_prob]\n",
        "actual_class=[list(training_set.class_indices.keys())[i.argmax()]for i in y_act]\n",
        "out_df=pd.DataFrame(np.vstack([predicted_class,actual_class]).T,columns=['predicted_class','actual_class'])\n",
        "confusion_matrix=pd.crosstab(out_df['actual_class'],out_df['predicted_class'],rownames=['Actual'],colnames=['Predicted'])\n",
        "import matplotlib.pyplot as plt\n",
        "sn.heatmap(confusion_matrix,cmap='flare', annot=True, fmt='d')\n",
        "plt.show()\n",
        "#plt.savefig('/content/drive/MyDrive/model weights/vgg16_AugGfb_split1_maxval_3')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lfe3JvsHa3iw"
      },
      "outputs": [],
      "source": [
        "lst=[r.history['loss'],r.history['val_loss'],r.history['accuracy'],r.history['val_accuracy']]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 222
        },
        "id": "_L24-mh_7XJ7",
        "outputId": "be032770-ae95-4df5-b7ce-c54e99b177f0"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>0</th>\n",
              "      <th>1</th>\n",
              "      <th>2</th>\n",
              "      <th>3</th>\n",
              "      <th>4</th>\n",
              "      <th>5</th>\n",
              "      <th>6</th>\n",
              "      <th>7</th>\n",
              "      <th>8</th>\n",
              "      <th>9</th>\n",
              "      <th>10</th>\n",
              "      <th>11</th>\n",
              "      <th>12</th>\n",
              "      <th>13</th>\n",
              "      <th>14</th>\n",
              "      <th>15</th>\n",
              "      <th>16</th>\n",
              "      <th>17</th>\n",
              "      <th>18</th>\n",
              "      <th>19</th>\n",
              "      <th>20</th>\n",
              "      <th>21</th>\n",
              "      <th>22</th>\n",
              "      <th>23</th>\n",
              "      <th>24</th>\n",
              "      <th>25</th>\n",
              "      <th>26</th>\n",
              "      <th>27</th>\n",
              "      <th>28</th>\n",
              "      <th>29</th>\n",
              "      <th>30</th>\n",
              "      <th>31</th>\n",
              "      <th>32</th>\n",
              "      <th>33</th>\n",
              "      <th>34</th>\n",
              "      <th>35</th>\n",
              "      <th>36</th>\n",
              "      <th>37</th>\n",
              "      <th>38</th>\n",
              "      <th>39</th>\n",
              "      <th>...</th>\n",
              "      <th>120</th>\n",
              "      <th>121</th>\n",
              "      <th>122</th>\n",
              "      <th>123</th>\n",
              "      <th>124</th>\n",
              "      <th>125</th>\n",
              "      <th>126</th>\n",
              "      <th>127</th>\n",
              "      <th>128</th>\n",
              "      <th>129</th>\n",
              "      <th>130</th>\n",
              "      <th>131</th>\n",
              "      <th>132</th>\n",
              "      <th>133</th>\n",
              "      <th>134</th>\n",
              "      <th>135</th>\n",
              "      <th>136</th>\n",
              "      <th>137</th>\n",
              "      <th>138</th>\n",
              "      <th>139</th>\n",
              "      <th>140</th>\n",
              "      <th>141</th>\n",
              "      <th>142</th>\n",
              "      <th>143</th>\n",
              "      <th>144</th>\n",
              "      <th>145</th>\n",
              "      <th>146</th>\n",
              "      <th>147</th>\n",
              "      <th>148</th>\n",
              "      <th>149</th>\n",
              "      <th>150</th>\n",
              "      <th>151</th>\n",
              "      <th>152</th>\n",
              "      <th>153</th>\n",
              "      <th>154</th>\n",
              "      <th>155</th>\n",
              "      <th>156</th>\n",
              "      <th>157</th>\n",
              "      <th>158</th>\n",
              "      <th>159</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>2.869423</td>\n",
              "      <td>1.804300</td>\n",
              "      <td>1.703288</td>\n",
              "      <td>1.484288</td>\n",
              "      <td>1.407032</td>\n",
              "      <td>1.582208</td>\n",
              "      <td>1.432651</td>\n",
              "      <td>1.267886</td>\n",
              "      <td>1.350396</td>\n",
              "      <td>1.179225</td>\n",
              "      <td>1.283551</td>\n",
              "      <td>1.164104</td>\n",
              "      <td>1.323075</td>\n",
              "      <td>1.163417</td>\n",
              "      <td>1.281835</td>\n",
              "      <td>1.125181</td>\n",
              "      <td>1.057142</td>\n",
              "      <td>1.175921</td>\n",
              "      <td>1.192630</td>\n",
              "      <td>1.204210</td>\n",
              "      <td>1.115507</td>\n",
              "      <td>1.066585</td>\n",
              "      <td>1.026756</td>\n",
              "      <td>1.154116</td>\n",
              "      <td>0.985939</td>\n",
              "      <td>1.152268</td>\n",
              "      <td>1.020498</td>\n",
              "      <td>1.057915</td>\n",
              "      <td>1.381507</td>\n",
              "      <td>0.894095</td>\n",
              "      <td>1.012724</td>\n",
              "      <td>0.951464</td>\n",
              "      <td>1.004424</td>\n",
              "      <td>0.940791</td>\n",
              "      <td>1.194896</td>\n",
              "      <td>1.087017</td>\n",
              "      <td>0.885508</td>\n",
              "      <td>0.902468</td>\n",
              "      <td>1.070229</td>\n",
              "      <td>0.949711</td>\n",
              "      <td>...</td>\n",
              "      <td>0.710397</td>\n",
              "      <td>0.765244</td>\n",
              "      <td>0.716947</td>\n",
              "      <td>0.574929</td>\n",
              "      <td>0.594573</td>\n",
              "      <td>0.591380</td>\n",
              "      <td>0.866335</td>\n",
              "      <td>0.757995</td>\n",
              "      <td>0.727700</td>\n",
              "      <td>0.594640</td>\n",
              "      <td>0.619796</td>\n",
              "      <td>0.676708</td>\n",
              "      <td>0.719936</td>\n",
              "      <td>0.729068</td>\n",
              "      <td>0.669281</td>\n",
              "      <td>0.638376</td>\n",
              "      <td>0.727684</td>\n",
              "      <td>0.606521</td>\n",
              "      <td>0.638911</td>\n",
              "      <td>0.843841</td>\n",
              "      <td>0.944651</td>\n",
              "      <td>0.573405</td>\n",
              "      <td>0.917740</td>\n",
              "      <td>0.905083</td>\n",
              "      <td>0.660692</td>\n",
              "      <td>0.574445</td>\n",
              "      <td>0.644448</td>\n",
              "      <td>0.634047</td>\n",
              "      <td>0.640869</td>\n",
              "      <td>0.535481</td>\n",
              "      <td>0.846208</td>\n",
              "      <td>0.628388</td>\n",
              "      <td>0.661492</td>\n",
              "      <td>0.796838</td>\n",
              "      <td>0.606268</td>\n",
              "      <td>0.665078</td>\n",
              "      <td>0.643782</td>\n",
              "      <td>0.839710</td>\n",
              "      <td>0.846258</td>\n",
              "      <td>0.668104</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1.261816</td>\n",
              "      <td>1.391285</td>\n",
              "      <td>2.233489</td>\n",
              "      <td>1.645025</td>\n",
              "      <td>1.396253</td>\n",
              "      <td>1.824860</td>\n",
              "      <td>1.475054</td>\n",
              "      <td>1.118352</td>\n",
              "      <td>1.187162</td>\n",
              "      <td>1.662094</td>\n",
              "      <td>1.081624</td>\n",
              "      <td>1.098304</td>\n",
              "      <td>1.000320</td>\n",
              "      <td>1.708298</td>\n",
              "      <td>1.884966</td>\n",
              "      <td>0.941100</td>\n",
              "      <td>1.033892</td>\n",
              "      <td>1.328767</td>\n",
              "      <td>1.313453</td>\n",
              "      <td>1.240766</td>\n",
              "      <td>1.040286</td>\n",
              "      <td>0.846798</td>\n",
              "      <td>1.726385</td>\n",
              "      <td>0.839719</td>\n",
              "      <td>0.912435</td>\n",
              "      <td>1.098501</td>\n",
              "      <td>1.044576</td>\n",
              "      <td>1.032382</td>\n",
              "      <td>0.947645</td>\n",
              "      <td>0.904537</td>\n",
              "      <td>1.133282</td>\n",
              "      <td>0.959801</td>\n",
              "      <td>1.089944</td>\n",
              "      <td>1.424622</td>\n",
              "      <td>1.428304</td>\n",
              "      <td>1.254242</td>\n",
              "      <td>0.995542</td>\n",
              "      <td>1.261873</td>\n",
              "      <td>0.928183</td>\n",
              "      <td>2.246228</td>\n",
              "      <td>...</td>\n",
              "      <td>0.757306</td>\n",
              "      <td>1.081612</td>\n",
              "      <td>0.638820</td>\n",
              "      <td>0.832434</td>\n",
              "      <td>0.688142</td>\n",
              "      <td>0.755485</td>\n",
              "      <td>0.757232</td>\n",
              "      <td>0.819963</td>\n",
              "      <td>0.640568</td>\n",
              "      <td>0.944299</td>\n",
              "      <td>0.769298</td>\n",
              "      <td>0.776725</td>\n",
              "      <td>0.982369</td>\n",
              "      <td>0.853733</td>\n",
              "      <td>0.762245</td>\n",
              "      <td>0.901339</td>\n",
              "      <td>0.787865</td>\n",
              "      <td>0.669449</td>\n",
              "      <td>1.283528</td>\n",
              "      <td>0.917625</td>\n",
              "      <td>0.704668</td>\n",
              "      <td>1.018072</td>\n",
              "      <td>1.098369</td>\n",
              "      <td>1.638518</td>\n",
              "      <td>0.978841</td>\n",
              "      <td>1.096704</td>\n",
              "      <td>0.899523</td>\n",
              "      <td>0.949629</td>\n",
              "      <td>0.785845</td>\n",
              "      <td>0.841954</td>\n",
              "      <td>0.851926</td>\n",
              "      <td>0.793916</td>\n",
              "      <td>0.999514</td>\n",
              "      <td>0.987346</td>\n",
              "      <td>1.058963</td>\n",
              "      <td>0.812826</td>\n",
              "      <td>0.718272</td>\n",
              "      <td>0.797232</td>\n",
              "      <td>0.756547</td>\n",
              "      <td>1.154406</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0.297149</td>\n",
              "      <td>0.357953</td>\n",
              "      <td>0.378908</td>\n",
              "      <td>0.404328</td>\n",
              "      <td>0.422879</td>\n",
              "      <td>0.417039</td>\n",
              "      <td>0.452078</td>\n",
              "      <td>0.488492</td>\n",
              "      <td>0.465819</td>\n",
              "      <td>0.520783</td>\n",
              "      <td>0.500515</td>\n",
              "      <td>0.530058</td>\n",
              "      <td>0.495019</td>\n",
              "      <td>0.532807</td>\n",
              "      <td>0.520440</td>\n",
              "      <td>0.536242</td>\n",
              "      <td>0.571968</td>\n",
              "      <td>0.555823</td>\n",
              "      <td>0.545517</td>\n",
              "      <td>0.552387</td>\n",
              "      <td>0.568190</td>\n",
              "      <td>0.581931</td>\n",
              "      <td>0.591893</td>\n",
              "      <td>0.578152</td>\n",
              "      <td>0.602199</td>\n",
              "      <td>0.560632</td>\n",
              "      <td>0.598420</td>\n",
              "      <td>0.588801</td>\n",
              "      <td>0.558227</td>\n",
              "      <td>0.646170</td>\n",
              "      <td>0.620405</td>\n",
              "      <td>0.632085</td>\n",
              "      <td>0.612504</td>\n",
              "      <td>0.630024</td>\n",
              "      <td>0.567503</td>\n",
              "      <td>0.610443</td>\n",
              "      <td>0.648574</td>\n",
              "      <td>0.642734</td>\n",
              "      <td>0.623497</td>\n",
              "      <td>0.635520</td>\n",
              "      <td>...</td>\n",
              "      <td>0.742013</td>\n",
              "      <td>0.723806</td>\n",
              "      <td>0.736860</td>\n",
              "      <td>0.775335</td>\n",
              "      <td>0.771900</td>\n",
              "      <td>0.784610</td>\n",
              "      <td>0.707661</td>\n",
              "      <td>0.729990</td>\n",
              "      <td>0.725524</td>\n",
              "      <td>0.774304</td>\n",
              "      <td>0.764686</td>\n",
              "      <td>0.745448</td>\n",
              "      <td>0.725867</td>\n",
              "      <td>0.726554</td>\n",
              "      <td>0.746479</td>\n",
              "      <td>0.760563</td>\n",
              "      <td>0.733081</td>\n",
              "      <td>0.767090</td>\n",
              "      <td>0.760563</td>\n",
              "      <td>0.704569</td>\n",
              "      <td>0.701134</td>\n",
              "      <td>0.782892</td>\n",
              "      <td>0.692202</td>\n",
              "      <td>0.694950</td>\n",
              "      <td>0.751632</td>\n",
              "      <td>0.781518</td>\n",
              "      <td>0.760907</td>\n",
              "      <td>0.774991</td>\n",
              "      <td>0.761250</td>\n",
              "      <td>0.802817</td>\n",
              "      <td>0.715562</td>\n",
              "      <td>0.773274</td>\n",
              "      <td>0.749227</td>\n",
              "      <td>0.726211</td>\n",
              "      <td>0.790794</td>\n",
              "      <td>0.743731</td>\n",
              "      <td>0.765373</td>\n",
              "      <td>0.723806</td>\n",
              "      <td>0.721402</td>\n",
              "      <td>0.761594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0.444043</td>\n",
              "      <td>0.361011</td>\n",
              "      <td>0.309266</td>\n",
              "      <td>0.347774</td>\n",
              "      <td>0.398315</td>\n",
              "      <td>0.318893</td>\n",
              "      <td>0.416366</td>\n",
              "      <td>0.500602</td>\n",
              "      <td>0.484958</td>\n",
              "      <td>0.388688</td>\n",
              "      <td>0.567990</td>\n",
              "      <td>0.537906</td>\n",
              "      <td>0.563177</td>\n",
              "      <td>0.474128</td>\n",
              "      <td>0.353791</td>\n",
              "      <td>0.596871</td>\n",
              "      <td>0.574007</td>\n",
              "      <td>0.471721</td>\n",
              "      <td>0.512635</td>\n",
              "      <td>0.484958</td>\n",
              "      <td>0.575211</td>\n",
              "      <td>0.671480</td>\n",
              "      <td>0.530686</td>\n",
              "      <td>0.652226</td>\n",
              "      <td>0.613718</td>\n",
              "      <td>0.586041</td>\n",
              "      <td>0.561974</td>\n",
              "      <td>0.598075</td>\n",
              "      <td>0.631769</td>\n",
              "      <td>0.623345</td>\n",
              "      <td>0.545126</td>\n",
              "      <td>0.611312</td>\n",
              "      <td>0.598075</td>\n",
              "      <td>0.481348</td>\n",
              "      <td>0.515042</td>\n",
              "      <td>0.543923</td>\n",
              "      <td>0.641396</td>\n",
              "      <td>0.628159</td>\n",
              "      <td>0.652226</td>\n",
              "      <td>0.466907</td>\n",
              "      <td>...</td>\n",
              "      <td>0.725632</td>\n",
              "      <td>0.663057</td>\n",
              "      <td>0.777377</td>\n",
              "      <td>0.720818</td>\n",
              "      <td>0.767750</td>\n",
              "      <td>0.726835</td>\n",
              "      <td>0.731649</td>\n",
              "      <td>0.716005</td>\n",
              "      <td>0.774970</td>\n",
              "      <td>0.682310</td>\n",
              "      <td>0.736462</td>\n",
              "      <td>0.718412</td>\n",
              "      <td>0.645006</td>\n",
              "      <td>0.712395</td>\n",
              "      <td>0.749699</td>\n",
              "      <td>0.678700</td>\n",
              "      <td>0.736462</td>\n",
              "      <td>0.768953</td>\n",
              "      <td>0.572804</td>\n",
              "      <td>0.696751</td>\n",
              "      <td>0.767750</td>\n",
              "      <td>0.654633</td>\n",
              "      <td>0.660650</td>\n",
              "      <td>0.558363</td>\n",
              "      <td>0.689531</td>\n",
              "      <td>0.634176</td>\n",
              "      <td>0.714801</td>\n",
              "      <td>0.695548</td>\n",
              "      <td>0.743682</td>\n",
              "      <td>0.706378</td>\n",
              "      <td>0.712395</td>\n",
              "      <td>0.746089</td>\n",
              "      <td>0.694344</td>\n",
              "      <td>0.682310</td>\n",
              "      <td>0.649819</td>\n",
              "      <td>0.705175</td>\n",
              "      <td>0.776173</td>\n",
              "      <td>0.736462</td>\n",
              "      <td>0.749699</td>\n",
              "      <td>0.691937</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "<p>4 rows × 160 columns</p>\n",
              "</div>"
            ],
            "text/plain": [
              "        0         1         2    ...       157       158       159\n",
              "0  2.869423  1.804300  1.703288  ...  0.839710  0.846258  0.668104\n",
              "1  1.261816  1.391285  2.233489  ...  0.797232  0.756547  1.154406\n",
              "2  0.297149  0.357953  0.378908  ...  0.723806  0.721402  0.761594\n",
              "3  0.444043  0.361011  0.309266  ...  0.736462  0.749699  0.691937\n",
              "\n",
              "[4 rows x 160 columns]"
            ]
          },
          "execution_count": 14,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import pandas as pd\n",
        "df=pd.DataFrame(lst)\n",
        "df"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LYsba_M_7XNO"
      },
      "outputs": [],
      "source": [
        "df.to_csv('RestNet.csv')"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
