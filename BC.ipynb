{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"BC.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"},"accelerator":"GPU","gpuClass":"standard"},"cells":[{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"O_r3UhjO7xyl","outputId":"fb656426-a9b3-493b-8fa9-e5d139e4ba08","executionInfo":{"status":"ok","timestamp":1659236452773,"user_tz":-360,"elapsed":10,"user":{"displayName":"Kaniz Fatema 191-15-12344","userId":"12749516128728889414"}}},"source":["!nvidia-smi"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Sun Jul 31 03:00:49 2022       \n","+-----------------------------------------------------------------------------+\n","| NVIDIA-SMI 460.32.03    Driver Version: 460.32.03    CUDA Version: 11.2     |\n","|-------------------------------+----------------------+----------------------+\n","| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n","| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n","|                               |                      |               MIG M. |\n","|===============================+======================+======================|\n","|   0  Tesla T4            Off  | 00000000:00:04.0 Off |                    0 |\n","| N/A   59C    P8    10W /  70W |      0MiB / 15109MiB |      0%      Default |\n","|                               |                      |                  N/A |\n","+-------------------------------+----------------------+----------------------+\n","                                                                               \n","+-----------------------------------------------------------------------------+\n","| Processes:                                                                  |\n","|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n","|        ID   ID                                                   Usage      |\n","|=============================================================================|\n","|  No running processes found                                                 |\n","+-----------------------------------------------------------------------------+\n"]}]},{"cell_type":"code","metadata":{"id":"ZAS0t-Q-NhGe","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1659236495572,"user_tz":-360,"elapsed":38207,"user":{"displayName":"Kaniz Fatema 191-15-12344","userId":"12749516128728889414"}},"outputId":"ab64a2a8-90a0-46c6-8d16-9904dfbb2174"},"source":["from google.colab import drive\n","drive.mount('/content/drive', force_remount=True)"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n"]}]},{"cell_type":"code","source":["! pip uninstall tensorflow \n","! pip install tensorflow==2.3.0"],"metadata":{"colab":{"base_uri":"https://localhost:8080/","height":1000},"id":"k3KwCKKfEDEu","executionInfo":{"status":"ok","timestamp":1659236573662,"user_tz":-360,"elapsed":73640,"user":{"displayName":"Kaniz Fatema 191-15-12344","userId":"12749516128728889414"}},"outputId":"2b21620b-b661-4579-fe0c-39ba4d2021c1"},"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Found existing installation: tensorflow 2.8.2+zzzcolab20220719082949\n","Uninstalling tensorflow-2.8.2+zzzcolab20220719082949:\n","  Would remove:\n","    /usr/local/bin/estimator_ckpt_converter\n","    /usr/local/bin/import_pb_to_tensorboard\n","    /usr/local/bin/saved_model_cli\n","    /usr/local/bin/tensorboard\n","    /usr/local/bin/tf_upgrade_v2\n","    /usr/local/bin/tflite_convert\n","    /usr/local/bin/toco\n","    /usr/local/bin/toco_from_protos\n","    /usr/local/lib/python3.7/dist-packages/tensorflow-2.8.2+zzzcolab20220719082949.dist-info/*\n","    /usr/local/lib/python3.7/dist-packages/tensorflow/*\n","Proceed (y/n)? y\n","  Successfully uninstalled tensorflow-2.8.2+zzzcolab20220719082949\n","y\n","Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n","Collecting tensorflow==2.3.0\n","  Downloading tensorflow-2.3.0-cp37-cp37m-manylinux2010_x86_64.whl (320.4 MB)\n","\u001b[K     |████████████████████████████████| 320.4 MB 54 kB/s \n","\u001b[?25hRequirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.47.0)\n","Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.2.0)\n","Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.3.0)\n","Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.0)\n","Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (2.8.0)\n","Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.15.0)\n","Collecting scipy==1.4.1\n","  Downloading scipy-1.4.1-cp37-cp37m-manylinux1_x86_64.whl (26.1 MB)\n","\u001b[K     |████████████████████████████████| 26.1 MB 1.3 MB/s \n","\u001b[?25hRequirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (3.17.3)\n","Collecting numpy<1.19.0,>=1.16.0\n","  Downloading numpy-1.18.5-cp37-cp37m-manylinux1_x86_64.whl (20.1 MB)\n","\u001b[K     |████████████████████████████████| 20.1 MB 1.1 MB/s \n","\u001b[?25hRequirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.14.1)\n","Collecting gast==0.3.3\n","  Downloading gast-0.3.3-py2.py3-none-any.whl (9.7 kB)\n","Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.1.2)\n","Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.37.1)\n","Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (0.2.0)\n","Collecting tensorflow-estimator<2.4.0,>=2.3.0\n","  Downloading tensorflow_estimator-2.3.0-py2.py3-none-any.whl (459 kB)\n","\u001b[K     |████████████████████████████████| 459 kB 64.3 MB/s \n","\u001b[?25hRequirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorflow==2.3.0) (1.6.3)\n","Collecting h5py<2.11.0,>=2.10.0\n","  Downloading h5py-2.10.0-cp37-cp37m-manylinux1_x86_64.whl (2.9 MB)\n","\u001b[K     |████████████████████████████████| 2.9 MB 19.5 MB/s \n","\u001b[?25hRequirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (57.4.0)\n","Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.0.1)\n","Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.4.1)\n","Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.8.1)\n","Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.6)\n","Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.35.0)\n","Requirement already satisfied: tensorboard-data-server<0.7.0,>=0.6.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.6.1)\n","Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.7/dist-packages (from tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.23.0)\n","Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.2.4)\n","Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.9)\n","Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.7/dist-packages (from google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.2.8)\n","Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.7/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.3.1)\n","Requirement already satisfied: importlib-metadata>=4.4 in /usr/local/lib/python3.7/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.12.0)\n","Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.8.1)\n","Requirement already satisfied: typing-extensions>=3.6.4 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=4.4->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (4.1.1)\n","Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /usr/local/lib/python3.7/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (0.4.8)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2022.6.15)\n","Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.0.4)\n","Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (1.24.3)\n","Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (2.10)\n","Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.7/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow==2.3.0) (3.2.0)\n","Installing collected packages: numpy, tensorflow-estimator, scipy, h5py, gast, tensorflow\n","  Attempting uninstall: numpy\n","    Found existing installation: numpy 1.21.6\n","    Uninstalling numpy-1.21.6:\n","      Successfully uninstalled numpy-1.21.6\n","  Attempting uninstall: tensorflow-estimator\n","    Found existing installation: tensorflow-estimator 2.8.0\n","    Uninstalling tensorflow-estimator-2.8.0:\n","      Successfully uninstalled tensorflow-estimator-2.8.0\n","  Attempting uninstall: scipy\n","    Found existing installation: scipy 1.7.3\n","    Uninstalling scipy-1.7.3:\n","      Successfully uninstalled scipy-1.7.3\n","  Attempting uninstall: h5py\n","    Found existing installation: h5py 3.1.0\n","    Uninstalling h5py-3.1.0:\n","      Successfully uninstalled h5py-3.1.0\n","  Attempting uninstall: gast\n","    Found existing installation: gast 0.5.3\n","    Uninstalling gast-0.5.3:\n","      Successfully uninstalled gast-0.5.3\n","\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n","xarray-einstats 0.2.2 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n","tables 3.7.0 requires numpy>=1.19.0, but you have numpy 1.18.5 which is incompatible.\n","pymc3 3.11.5 requires scipy<1.8.0,>=1.7.3, but you have scipy 1.4.1 which is incompatible.\n","jaxlib 0.3.14+cuda11.cudnn805 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jaxlib 0.3.14+cuda11.cudnn805 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n","jax 0.3.14 requires numpy>=1.19, but you have numpy 1.18.5 which is incompatible.\n","jax 0.3.14 requires scipy>=1.5, but you have scipy 1.4.1 which is incompatible.\n","datascience 0.10.6 requires folium==0.2.1, but you have folium 0.8.3 which is incompatible.\n","cmdstanpy 1.0.4 requires numpy>=1.21, but you have numpy 1.18.5 which is incompatible.\n","albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n","Successfully installed gast-0.3.3 h5py-2.10.0 numpy-1.18.5 scipy-1.4.1 tensorflow-2.3.0 tensorflow-estimator-2.3.0\n"]},{"output_type":"display_data","data":{"application/vnd.colab-display-data+json":{"pip_warning":{"packages":["numpy"]}}},"metadata":{}}]},{"cell_type":"code","metadata":{"id":"EqcZPTT0J8w1"},"source":["import os\n","import numpy as np\n","import cv2\n","from glob import glob\n","from matplotlib import pyplot\n","from sklearn.utils import shuffle\n","import tensorflow as tf\n","from tensorflow.keras.layers import *\n","from tensorflow.keras.models import Model\n","from tensorflow.keras.optimizers import Adam"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"KL7ogre7KXAm"},"source":["IMG_H = 224\n","IMG_W = 224\n","IMG_C = 1  ## Change this to 1 for grayscale.\n","w_init = tf.keras.initializers.RandomNormal(mean=0.0, stddev=0.02)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"JuTRGLnsKZm6"},"source":["def load_image(image_path):\n","    img = tf.io.read_file(image_path)\n","    img = tf.io.decode_jpeg(img)\n","    img = tf.image.resize_with_crop_or_pad(img, IMG_H, IMG_W)\n","    img = tf.cast(img, tf.float32)\n","    img = (img - 127.5) / 127.5\n","    return img\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"SBEHxAwwKcHj"},"source":["def tf_dataset(images_path, batch_size):\n","    dataset = tf.data.Dataset.from_tensor_slices(images_path)\n","    dataset = dataset.shuffle(buffer_size=10240)\n","    dataset = dataset.map(load_image, num_parallel_calls=tf.data.experimental.AUTOTUNE)\n","    dataset = dataset.batch(batch_size)\n","    dataset = dataset.prefetch(buffer_size=tf.data.experimental.AUTOTUNE)\n","    return dataset\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XvsHOPhdKdUj"},"source":["def deconv_block(inputs, num_filters, kernel_size, strides, bn=True):\n","    x = Conv2DTranspose(\n","        filters=num_filters,\n","        kernel_size=kernel_size,\n","        kernel_initializer=w_init,\n","        padding=\"same\",\n","        strides=strides,\n","        use_bias=False\n","        )(inputs)\n","\n","    if bn:\n","        x = BatchNormalization()(x)\n","        x = LeakyReLU(alpha=0.2)(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"n2iDAC84KgKV"},"source":["def conv_block(inputs, num_filters, kernel_size, padding=\"same\", strides=2, activation=True):\n","    x = Conv2D(\n","        filters=num_filters,\n","        kernel_size=kernel_size,\n","        kernel_initializer=w_init,\n","        padding=padding,\n","        strides=strides,\n","    )(inputs)\n","\n","    if activation:\n","        x = LeakyReLU(alpha=0.2)(x)\n","        x = Dropout(0.3)(x)\n","    return x"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"tN1L78QfKnz7"},"source":["def build_generator(latent_dim):\n","    f = [2**i for i in range(5)][::-1]\n","    filters = 32\n","    output_strides = 16\n","    h_output = IMG_H // output_strides\n","    w_output = IMG_W // output_strides\n","\n","    noise = Input(shape=(latent_dim,), name=\"generator_noise_input\")\n","\n","    x = Dense(f[0] * filters * h_output * w_output, use_bias=False)(noise)\n","    x = BatchNormalization()(x)\n","    x = LeakyReLU(alpha=0.2)(x)\n","    x = Reshape((h_output, w_output, 16 * filters))(x)\n","\n","    for i in range(1, 5):\n","        x = deconv_block(x,\n","            num_filters=f[i] * filters,\n","            kernel_size=5,\n","            strides=2,\n","            bn=True\n","        )\n","\n","    x = conv_block(x,\n","        num_filters=1,  ## Change this to 1 for grayscale.\n","        kernel_size=5,\n","        strides=1,\n","        activation=False\n","    )\n","    fake_output = Activation(\"tanh\")(x)\n","\n","    return Model(noise, fake_output, name=\"generator\")\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"7Fv0RLnrKwbT"},"source":["def build_discriminator():\n","    f = [2**i for i in range(4)]\n","    image_input = Input(shape=(IMG_H, IMG_W, IMG_C))\n","    x = image_input\n","    filters = 64\n","    output_strides = 16\n","    h_output = IMG_H // output_strides\n","    w_output = IMG_W // output_strides\n","\n","    for i in range(0, 4):\n","        x = conv_block(x, num_filters=f[i] * filters, kernel_size=5, strides=2)\n","\n","    x = Flatten()(x)\n","    x = Dense(1)(x)\n","\n","    return Model(image_input, x, name=\"discriminator\")\n","\n","class GAN(Model):\n","    def __init__(self, discriminator, generator, latent_dim):\n","        super(GAN, self).__init__()\n","        self.discriminator = discriminator\n","        self.generator = generator\n","        self.latent_dim = latent_dim\n","\n","    def compile(self, d_optimizer, g_optimizer, loss_fn):\n","        super(GAN, self).compile()\n","        self.d_optimizer = d_optimizer\n","        self.g_optimizer = g_optimizer\n","        self.loss_fn = loss_fn\n","\n","    def train_step(self, real_images):\n","        batch_size = tf.shape(real_images)[0]\n","\n","        for _ in range(2):\n","            ## Train the discriminator\n","            random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","            generated_images = self.generator(random_latent_vectors)\n","            generated_labels = tf.zeros((batch_size, 1))\n","\n","            with tf.GradientTape() as ftape:\n","                predictions = self.discriminator(generated_images)\n","                d1_loss = self.loss_fn(generated_labels, predictions)\n","            grads = ftape.gradient(d1_loss, self.discriminator.trainable_weights)\n","            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","\n","            ## Train the discriminator\n","            labels = tf.ones((batch_size, 1))\n","\n","            with tf.GradientTape() as rtape:\n","                predictions = self.discriminator(real_images)\n","                d2_loss = self.loss_fn(labels, predictions)\n","            grads = rtape.gradient(d2_loss, self.discriminator.trainable_weights)\n","            self.d_optimizer.apply_gradients(zip(grads, self.discriminator.trainable_weights))\n","\n","        ## Train the generator\n","        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n","        misleading_labels = tf.ones((batch_size, 1))\n","\n","        with tf.GradientTape() as gtape:\n","            predictions = self.discriminator(self.generator(random_latent_vectors))\n","            g_loss = self.loss_fn(misleading_labels, predictions)\n","        grads = gtape.gradient(g_loss, self.generator.trainable_weights)\n","        self.g_optimizer.apply_gradients(zip(grads, self.generator.trainable_weights))\n","\n","        return {\"d1_loss\": d1_loss, \"d2_loss\": d2_loss, \"g_loss\": g_loss}\n","\n","def save_plot(examples, epoch, n):\n","    examples = (examples + 1) / 2.0\n","    for i in range(n * n):\n","        pyplot.subplot(n, n, i+1)\n","        pyplot.axis(\"off\")\n","        #pyplot.imshow(examples[i])  ## pyplot.imshow(np.squeeze(examples[i], axis=-1))\n","        pyplot.imshow(examples[i, :,:,0], cmap='gray')\n","    filename = f\"/content/drive/MyDrive/Experiment/GAN/Output/n-{epoch+1}.png\"\n","    pyplot.savefig(filename,bbox_inches='tight', pad_inches = 0)\n","    pyplot.close()\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"xQAGjvrYK4ZR","outputId":"b3ef7748-4da8-49c1-d2e8-49ddecb87eb8"},"source":["if __name__ == \"__main__\":\n","    ## Hyperparameters\n","    batch_size = 128\n","    latent_dim = 128\n","    num_epochs = 250\n","    images_path = glob(\"/content/drive/MyDrive/Experiment/Knee/knee(mandeky)/0/*\")\n","\n","    d_model = build_discriminator()\n","    g_model = build_generator(latent_dim)\n","\n","     #d_model.load_weights(\"/content/drive/MyDrive/Dataset orginial/output/output/d_model.h5\")\n","     #g_model.load_weights(\"/content/drive/MyDrive/Dataset orginial/output/output/g_model.h5\")\n","\n","    d_model.summary()\n","    g_model.summary()\n","\n","    gan = GAN(d_model, g_model, latent_dim)\n","\n","    bce_loss_fn = tf.keras.losses.BinaryCrossentropy(from_logits=True, label_smoothing=0.1)\n","    d_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","    g_optimizer = tf.keras.optimizers.Adam(learning_rate=0.0002, beta_1=0.5)\n","    gan.compile(d_optimizer, g_optimizer, bce_loss_fn)\n","\n","    images_dataset = tf_dataset(images_path, batch_size)\n","\n","    for epoch in range(num_epochs):\n","        gan.fit(images_dataset, epochs=1)\n","        g_model.save(\"/content/drive/MyDrive/Experiment/GAN/nn/BCg_model.h5\")\n","        d_model.save(\"/content/drive/MyDrive/Experiment/GAN/nn/BCd_model.h5\")\n","\n","        n_samples = 1\n","        noise = np.random.normal(size=(n_samples, latent_dim))\n","        examples = g_model.predict(noise)\n","        save_plot(examples, epoch, int(np.sqrt(n_samples)))\n"],"execution_count":null,"outputs":[{"output_type":"stream","name":"stdout","text":["Model: \"discriminator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","input_2 (InputLayer)         [(None, 224, 224, 1)]     0         \n","_________________________________________________________________\n","conv2d_5 (Conv2D)            (None, 112, 112, 64)      1664      \n","_________________________________________________________________\n","leaky_re_lu_9 (LeakyReLU)    (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","dropout_4 (Dropout)          (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","conv2d_6 (Conv2D)            (None, 56, 56, 128)       204928    \n","_________________________________________________________________\n","leaky_re_lu_10 (LeakyReLU)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","dropout_5 (Dropout)          (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","conv2d_7 (Conv2D)            (None, 28, 28, 256)       819456    \n","_________________________________________________________________\n","leaky_re_lu_11 (LeakyReLU)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","dropout_6 (Dropout)          (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","conv2d_8 (Conv2D)            (None, 14, 14, 512)       3277312   \n","_________________________________________________________________\n","leaky_re_lu_12 (LeakyReLU)   (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","dropout_7 (Dropout)          (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","flatten_1 (Flatten)          (None, 100352)            0         \n","_________________________________________________________________\n","dense_2 (Dense)              (None, 1)                 100353    \n","=================================================================\n","Total params: 4,403,713\n","Trainable params: 4,403,713\n","Non-trainable params: 0\n","_________________________________________________________________\n","Model: \"generator\"\n","_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","generator_noise_input (Input [(None, 128)]             0         \n","_________________________________________________________________\n","dense_3 (Dense)              (None, 100352)            12845056  \n","_________________________________________________________________\n","batch_normalization_5 (Batch (None, 100352)            401408    \n","_________________________________________________________________\n","leaky_re_lu_13 (LeakyReLU)   (None, 100352)            0         \n","_________________________________________________________________\n","reshape_1 (Reshape)          (None, 14, 14, 512)       0         \n","_________________________________________________________________\n","conv2d_transpose_4 (Conv2DTr (None, 28, 28, 256)       3276800   \n","_________________________________________________________________\n","batch_normalization_6 (Batch (None, 28, 28, 256)       1024      \n","_________________________________________________________________\n","leaky_re_lu_14 (LeakyReLU)   (None, 28, 28, 256)       0         \n","_________________________________________________________________\n","conv2d_transpose_5 (Conv2DTr (None, 56, 56, 128)       819200    \n","_________________________________________________________________\n","batch_normalization_7 (Batch (None, 56, 56, 128)       512       \n","_________________________________________________________________\n","leaky_re_lu_15 (LeakyReLU)   (None, 56, 56, 128)       0         \n","_________________________________________________________________\n","conv2d_transpose_6 (Conv2DTr (None, 112, 112, 64)      204800    \n","_________________________________________________________________\n","batch_normalization_8 (Batch (None, 112, 112, 64)      256       \n","_________________________________________________________________\n","leaky_re_lu_16 (LeakyReLU)   (None, 112, 112, 64)      0         \n","_________________________________________________________________\n","conv2d_transpose_7 (Conv2DTr (None, 224, 224, 32)      51200     \n","_________________________________________________________________\n","batch_normalization_9 (Batch (None, 224, 224, 32)      128       \n","_________________________________________________________________\n","leaky_re_lu_17 (LeakyReLU)   (None, 224, 224, 32)      0         \n","_________________________________________________________________\n","conv2d_9 (Conv2D)            (None, 224, 224, 1)       801       \n","_________________________________________________________________\n","activation_1 (Activation)    (None, 224, 224, 1)       0         \n","=================================================================\n","Total params: 17,601,185\n","Trainable params: 17,399,521\n","Non-trainable params: 201,664\n","_________________________________________________________________\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2703 - d2_loss: 0.2251 - g_loss: 2.5350\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2506 - d2_loss: 0.2297 - g_loss: 2.4093\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2112 - d2_loss: 0.2104 - g_loss: 2.8903\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2014 - d2_loss: 0.2000 - g_loss: 2.7037\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2000 - d2_loss: 0.1992 - g_loss: 2.7286\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2014 - d2_loss: 0.1989 - g_loss: 2.7562\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2056 - d2_loss: 0.2152 - g_loss: 2.6393\n","26/26 [==============================] - 97s 4s/step - d1_loss: 12.0556 - d2_loss: 1.9221 - g_loss: 4.3523\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2521 - d2_loss: 0.2095 - g_loss: 3.5450\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2046 - d2_loss: 0.2052 - g_loss: 2.7250\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2095 - d2_loss: 0.2051 - g_loss: 2.4621\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2045 - d2_loss: 0.2030 - g_loss: 2.5241\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2074 - d2_loss: 0.2022 - g_loss: 2.4564\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2066 - d2_loss: 0.2012 - g_loss: 2.4765\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2056 - d2_loss: 0.2007 - g_loss: 2.5172\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2058 - d2_loss: 0.2005 - g_loss: 2.5293\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2049 - d2_loss: 0.2005 - g_loss: 2.6866\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2014 - d2_loss: 0.2007 - g_loss: 3.1489\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2108 - d2_loss: 0.2010 - g_loss: 2.4659\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2053 - d2_loss: 0.2002 - g_loss: 2.5341\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2056 - d2_loss: 0.2019 - g_loss: 2.6130\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2041 - d2_loss: 0.2001 - g_loss: 2.5557\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2031 - d2_loss: 0.1998 - g_loss: 2.5969\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2031 - d2_loss: 0.1999 - g_loss: 2.6190\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2040 - d2_loss: 0.1999 - g_loss: 2.5547\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2048 - d2_loss: 0.2004 - g_loss: 2.7033\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2014 - d2_loss: 0.2000 - g_loss: 2.6416\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2019 - d2_loss: 0.1998 - g_loss: 2.6385\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2013 - d2_loss: 0.1997 - g_loss: 2.6394\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2022 - d2_loss: 0.2000 - g_loss: 2.6768\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1994 - d2_loss: 0.1994 - g_loss: 2.9908\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1996 - d2_loss: 0.1995 - g_loss: 3.0495\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1995 - d2_loss: 0.1994 - g_loss: 3.0213\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1995 - d2_loss: 0.1995 - g_loss: 3.0342\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1997 - d2_loss: 0.1996 - g_loss: 3.0512\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1996 - d2_loss: 0.1998 - g_loss: 3.0193\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1999 - d2_loss: 0.2001 - g_loss: 3.0420\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1992 - d2_loss: 0.1993 - g_loss: 2.9931\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1992 - d2_loss: 0.1994 - g_loss: 3.0008\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1995 - d2_loss: 0.1997 - g_loss: 3.0370\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1993 - d2_loss: 0.1995 - g_loss: 3.0031\n","26/26 [==============================] - 97s 4s/step - d1_loss: 46.4354 - d2_loss: 3.9690 - g_loss: 37.1982\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.3114 - d2_loss: 0.4061 - g_loss: 4.7485\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2561 - d2_loss: 0.2272 - g_loss: 4.2077\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2174 - d2_loss: 0.2126 - g_loss: 3.1221\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2097 - d2_loss: 0.2062 - g_loss: 3.1231\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2032 - d2_loss: 0.2092 - g_loss: 2.8937\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2004 - d2_loss: 0.2050 - g_loss: 2.8703\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1994 - d2_loss: 0.2028 - g_loss: 2.8502\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1992 - d2_loss: 0.2014 - g_loss: 2.8583\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1996 - d2_loss: 0.2009 - g_loss: 2.8559\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1989 - d2_loss: 0.2009 - g_loss: 2.8434\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1991 - d2_loss: 0.2007 - g_loss: 2.8441\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1989 - d2_loss: 0.2005 - g_loss: 2.8394\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1990 - d2_loss: 0.2003 - g_loss: 2.8194\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2002 - d2_loss: 0.2008 - g_loss: 2.8404\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1996 - d2_loss: 0.2008 - g_loss: 2.8154\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2011 - d2_loss: 0.2019 - g_loss: 2.8343\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2021 - d2_loss: 0.2042 - g_loss: 2.7115\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2055 - d2_loss: 0.2036 - g_loss: 2.5769\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2008 - d2_loss: 0.2005 - g_loss: 2.7931\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1990 - d2_loss: 0.2000 - g_loss: 2.8115\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2066 - d2_loss: 0.2035 - g_loss: 2.5272\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1989 - d2_loss: 0.1996 - g_loss: 2.7909\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.1988 - d2_loss: 0.1994 - g_loss: 2.8192\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1991 - d2_loss: 0.2005 - g_loss: 2.8179\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2021 - d2_loss: 0.2032 - g_loss: 2.6598\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1990 - d2_loss: 0.1996 - g_loss: 2.7892\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1989 - d2_loss: 0.1997 - g_loss: 2.8289\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2044 - d2_loss: 0.2035 - g_loss: 2.6903\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1991 - d2_loss: 0.1995 - g_loss: 2.7867\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1993 - d2_loss: 0.2009 - g_loss: 2.8063\n","26/26 [==============================] - 97s 4s/step - d1_loss: 0.2031 - d2_loss: 0.2012 - g_loss: 2.7160\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1988 - d2_loss: 0.1997 - g_loss: 2.8694\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2003 - d2_loss: 0.2008 - g_loss: 3.1155\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2004 - d2_loss: 0.2007 - g_loss: 3.1308\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1988 - d2_loss: 0.1993 - g_loss: 2.9242\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2001 - d2_loss: 0.2005 - g_loss: 3.0857\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.2001 - d2_loss: 0.2005 - g_loss: 3.0952\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1995 - d2_loss: 0.2000 - g_loss: 3.0152\n","26/26 [==============================] - 96s 4s/step - d1_loss: 0.1996 - d2_loss: 0.2000 - g_loss: 3.0363\n","24/26 [==========================>...] - ETA: 7s - d1_loss: 0.1997 - d2_loss: 0.2002 - g_loss: 3.0507 "]}]},{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"id":"kI3bmifIkxJu"},"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"VKHIY-3_boAZ"},"source":[""],"execution_count":null,"outputs":[]}]}